{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees in Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will explore various techniques for preventing overfitting in decision trees. We will extend the implementation of the binary decision trees that we implemented in the previous assignment. You will have to use your solutions from this previous assignment and extend them.\n",
    "\n",
    "In this assignment you will:\n",
    "\n",
    "* Implement binary decision trees with different early stopping methods.\n",
    "* Compare models with different stopping parameters.\n",
    "* Visualize the concept of overfitting in decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Exploring Some Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanch\\anaconda3\\envs\\py37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (19,47) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>...</th>\n",
       "      <th>sub_grade_num</th>\n",
       "      <th>delinq_2yrs_zero</th>\n",
       "      <th>pub_rec_zero</th>\n",
       "      <th>collections_12_mths_zero</th>\n",
       "      <th>short_emp</th>\n",
       "      <th>payment_inc_ratio</th>\n",
       "      <th>final_d</th>\n",
       "      <th>last_delinq_none</th>\n",
       "      <th>last_record_none</th>\n",
       "      <th>last_major_derog_none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>4975</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.143500</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>2500</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.393200</td>\n",
       "      <td>20161201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>2400</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.259550</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.275850</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075269</td>\n",
       "      <td>1311441</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>156.46</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.215330</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122602</th>\n",
       "      <td>9856168</td>\n",
       "      <td>11708132</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>6000</td>\n",
       "      <td>60 months</td>\n",
       "      <td>23.40</td>\n",
       "      <td>170.53</td>\n",
       "      <td>E</td>\n",
       "      <td>E5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.487630</td>\n",
       "      <td>20190101T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122603</th>\n",
       "      <td>9795013</td>\n",
       "      <td>11647121</td>\n",
       "      <td>15250</td>\n",
       "      <td>15250</td>\n",
       "      <td>15250</td>\n",
       "      <td>36 months</td>\n",
       "      <td>17.57</td>\n",
       "      <td>548.05</td>\n",
       "      <td>D</td>\n",
       "      <td>D2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.117800</td>\n",
       "      <td>20170101T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122604</th>\n",
       "      <td>9695736</td>\n",
       "      <td>11547808</td>\n",
       "      <td>8525</td>\n",
       "      <td>8525</td>\n",
       "      <td>8525</td>\n",
       "      <td>60 months</td>\n",
       "      <td>18.25</td>\n",
       "      <td>217.65</td>\n",
       "      <td>D</td>\n",
       "      <td>D3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.958120</td>\n",
       "      <td>20190101T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122605</th>\n",
       "      <td>9684700</td>\n",
       "      <td>11536848</td>\n",
       "      <td>22000</td>\n",
       "      <td>22000</td>\n",
       "      <td>22000</td>\n",
       "      <td>60 months</td>\n",
       "      <td>19.97</td>\n",
       "      <td>582.50</td>\n",
       "      <td>D</td>\n",
       "      <td>D5</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.961540</td>\n",
       "      <td>20190101T000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122606</th>\n",
       "      <td>9604874</td>\n",
       "      <td>11457002</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>36 months</td>\n",
       "      <td>7.90</td>\n",
       "      <td>62.59</td>\n",
       "      <td>A</td>\n",
       "      <td>A4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.904916</td>\n",
       "      <td>20170101T000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122607 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  member_id  loan_amnt  funded_amnt  funded_amnt_inv  \\\n",
       "0       1077501    1296599       5000         5000             4975   \n",
       "1       1077430    1314167       2500         2500             2500   \n",
       "2       1077175    1313524       2400         2400             2400   \n",
       "3       1076863    1277178      10000        10000            10000   \n",
       "4       1075269    1311441       5000         5000             5000   \n",
       "...         ...        ...        ...          ...              ...   \n",
       "122602  9856168   11708132       6000         6000             6000   \n",
       "122603  9795013   11647121      15250        15250            15250   \n",
       "122604  9695736   11547808       8525         8525             8525   \n",
       "122605  9684700   11536848      22000        22000            22000   \n",
       "122606  9604874   11457002       2000         2000             2000   \n",
       "\n",
       "              term  int_rate  installment grade sub_grade  ... sub_grade_num  \\\n",
       "0        36 months     10.65       162.87     B        B2  ...           0.4   \n",
       "1        60 months     15.27        59.83     C        C4  ...           0.8   \n",
       "2        36 months     15.96        84.33     C        C5  ...           1.0   \n",
       "3        36 months     13.49       339.31     C        C1  ...           0.2   \n",
       "4        36 months      7.90       156.46     A        A4  ...           0.8   \n",
       "...            ...       ...          ...   ...       ...  ...           ...   \n",
       "122602   60 months     23.40       170.53     E        E5  ...           1.0   \n",
       "122603   36 months     17.57       548.05     D        D2  ...           0.4   \n",
       "122604   60 months     18.25       217.65     D        D3  ...           0.6   \n",
       "122605   60 months     19.97       582.50     D        D5  ...           1.0   \n",
       "122606   36 months      7.90        62.59     A        A4  ...           0.8   \n",
       "\n",
       "       delinq_2yrs_zero pub_rec_zero  collections_12_mths_zero short_emp  \\\n",
       "0                   1.0          1.0                       1.0         0   \n",
       "1                   1.0          1.0                       1.0         1   \n",
       "2                   1.0          1.0                       1.0         0   \n",
       "3                   1.0          1.0                       1.0         0   \n",
       "4                   1.0          1.0                       1.0         0   \n",
       "...                 ...          ...                       ...       ...   \n",
       "122602              0.0          1.0                       1.0         1   \n",
       "122603              0.0          0.0                       1.0         0   \n",
       "122604              0.0          1.0                       1.0         0   \n",
       "122605              1.0          0.0                       1.0         0   \n",
       "122606              0.0          1.0                       1.0         0   \n",
       "\n",
       "       payment_inc_ratio          final_d last_delinq_none last_record_none  \\\n",
       "0               8.143500  20141201T000000                1                1   \n",
       "1               2.393200  20161201T000000                1                1   \n",
       "2               8.259550  20141201T000000                1                1   \n",
       "3               8.275850  20141201T000000                0                1   \n",
       "4               5.215330  20141201T000000                1                1   \n",
       "...                  ...              ...              ...              ...   \n",
       "122602          4.487630  20190101T000000                0                1   \n",
       "122603         10.117800  20170101T000000                0                0   \n",
       "122604          6.958120  20190101T000000                0                1   \n",
       "122605          8.961540  20190101T000000                1                0   \n",
       "122606          0.904916  20170101T000000                0                1   \n",
       "\n",
       "       last_major_derog_none  \n",
       "0                          1  \n",
       "1                          1  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          1  \n",
       "...                      ...  \n",
       "122602                     0  \n",
       "122603                     0  \n",
       "122604                     0  \n",
       "122605                     1  \n",
       "122606                     1  \n",
       "\n",
       "[122607 rows x 68 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans = pd.read_csv('lending-club-data.csv')\n",
    "loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_loans =  1 => safe\n",
    "# safe_loans = -1 => risky\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans = loans.drop(['bad_loans'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same 4 categorical features as in the previous assignment: \n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment.\n",
    "\n",
    "In the dataset, each of these features is a categorical feature. Since we are building a binary decision tree, we will have to convert this to binary data in a subsequent section using 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We used `seed = 1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.5\n",
      "Percentage of risky loans                : 0.5\n",
      "Total number of loans in our new dataset : 46300\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == +1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Since there are fewer risky loans than safe loans, find the ratio of the sizes\n",
    "# and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(frac=percentage, random_state=1)\n",
    "\n",
    "# Append the risky_loans with the downsampled version of safe_loans\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "print(\"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data)))\n",
    "print(\"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data)))\n",
    "print(\"Total number of loans in our new dataset :\", len(loans_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grade</th>\n",
       "      <th>term</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>safe_loans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>F</td>\n",
       "      <td>60 months</td>\n",
       "      <td>OWN</td>\n",
       "      <td>4 years</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>3 years</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>A</td>\n",
       "      <td>36 months</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>3 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58367</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>7 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90431</th>\n",
       "      <td>B</td>\n",
       "      <td>36 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>8 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115727</th>\n",
       "      <td>F</td>\n",
       "      <td>60 months</td>\n",
       "      <td>RENT</td>\n",
       "      <td>3 years</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105752</th>\n",
       "      <td>D</td>\n",
       "      <td>60 months</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>1 year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       grade        term home_ownership emp_length  safe_loans\n",
       "1          C   60 months           RENT   < 1 year          -1\n",
       "6          F   60 months            OWN    4 years          -1\n",
       "7          B   60 months           RENT   < 1 year          -1\n",
       "10         C   36 months           RENT   < 1 year          -1\n",
       "12         B   36 months           RENT    3 years          -1\n",
       "...      ...         ...            ...        ...         ...\n",
       "10016      A   36 months       MORTGAGE    3 years           1\n",
       "58367      B   36 months       MORTGAGE    7 years           1\n",
       "90431      B   36 months           RENT    8 years           1\n",
       "115727     F   60 months           RENT    3 years           1\n",
       "105752     D   60 months       MORTGAGE     1 year           1\n",
       "\n",
       "[46300 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are implementing **binary decision trees**, we transform our categorical data into binary data using 1-hot encoding, just as in the previous assignment. Here is the summary of that discussion:\n",
    "\n",
    "For instance, the **home_ownership** feature represents the home ownership status of the loanee, which is either `own`, `mortgage` or `rent`. For example, if a data point has the feature \n",
    "```\n",
    "   {'home_ownership': 'RENT'}\n",
    "```\n",
    "we want to turn this into three features: \n",
    "```\n",
    " { \n",
    "   'home_ownership = OWN'      : 0, \n",
    "   'home_ownership = MORTGAGE' : 0, \n",
    "   'home_ownership = RENT'     : 1\n",
    " }\n",
    "```\n",
    "\n",
    "Since this code requires a few Python and Turi Create tricks, feel free to use this block of code as is. Refer to the API documentation for a deeper understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-assign-labels-with-sklearn-one-hot-encoder-e59a5f17df4f\n",
    "from sklearn.preprocessing import OneHotEncoder as SklearnOneHotEncoder\n",
    "\n",
    "class OneHotEncoder(SklearnOneHotEncoder):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(OneHotEncoder, self).__init__(**kwargs)\n",
    "        self.fit_flag = False\n",
    "\n",
    "    def fit(self, X, **kwargs):\n",
    "        out = super().fit(X)\n",
    "        self.fit_flag = True\n",
    "        return out\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        sparse_matrix = super(OneHotEncoder, self).transform(X)\n",
    "        new_columns = self.get_new_columns(X=X)\n",
    "        d_out = pd.DataFrame(sparse_matrix.toarray(), columns=new_columns, index=X.index)\n",
    "        return d_out\n",
    "\n",
    "    def fit_transform(self, X, **kwargs):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def get_new_columns(self, X):\n",
    "        new_columns = []\n",
    "        for i, column in enumerate(X.columns):\n",
    "            j = 0\n",
    "            while j < len(self.categories_[i]):\n",
    "                new_columns.append(f'{column}_<{self.categories_[i][j]}>')\n",
    "                j += 1\n",
    "        return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd\n",
    "#Using dummies values approach\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "for feature in features:\n",
    "    newfeature = pd.get_dummies(loans_data[feature], columns=[feature], prefix=feature )\n",
    "    loans_data = loans_data.join(newfeature)\n",
    "\n",
    "loans_data = loans_data.drop(features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split\n",
    "\n",
    "We split the data into a train-validation split with 80% of the data in the training set and 20% of the data in the validation set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_set = train_test_split(loans_data, train_size=.8, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping methods for decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will extend the **binary tree implementation** from the previous assignment in order to handle some early stopping conditions. Recall the 3 early stopping methods that were discussed in lecture:\n",
    "\n",
    "1. Reached a **maximum depth**. (set by parameter `max_depth`).\n",
    "2. Reached a **minimum node size**. (set by parameter `min_node_size`).\n",
    "3. Don't split if the **gain in error reduction** is too small. (set by parameter `min_error_reduction`).\n",
    "\n",
    "For the rest of this assignment, we will refer to these three as **early stopping conditions 1, 2, and 3**.\n",
    "\n",
    "### Early stopping condition 1: Maximum depth\n",
    "\n",
    "Recall that we already implemented the maximum depth stopping condition in the previous assignment. In this assignment, we will experiment with this condition a bit more and also write code to implement the 2nd and 3rd early stopping conditions.\n",
    "\n",
    "We will be reusing code from the previous assignment and then building upon this.  We will **alert you** when you reach a function that was part of the previous assignment so that you can simply copy and past your previous code.\n",
    "\n",
    "### Early stopping condition 2: Minimum node size\n",
    "The function **reached_minimum_node_size** takes 2 arguments:\n",
    "\n",
    "1. The `data` (from a node)\n",
    "2. The minimum number of data points that a node is allowed to split on, `min_node_size`.\n",
    "\n",
    "This function simply calculates whether the number of data points at a given node is less than or equal to the specified minimum node size. This function will be used to detect this early stopping condition in the **decision_tree_create** function.\n",
    "\n",
    "Fill in the parts of the function below where you find `## YOUR CODE HERE`.  There is **one** instance in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reached_minimum_node_size(data, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    ## YOUR CODE HERE\n",
    "    if len(data) <= min_node_size:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping condition 3: Minimum gain in error reduction\n",
    "The function **error_reduction** takes 2 arguments:\n",
    "\n",
    "1. The error **before** a split, `error_before_split`.\n",
    "2. The error **after** a split, `error_after_split`.\n",
    "\n",
    "This function computes the gain in error reduction, i.e., the difference between the error before the split and that after the split. This function will be used to detect this early stopping condition in the **decision_tree_create** function.\n",
    "\n",
    "Fill in the parts of the function below where you find `## YOUR CODE HERE`.  There is **one** instance in the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_reduction(error_before_split, error_after_split):\n",
    "    # Return the error before the split minus the error after the split.\n",
    "    ## YOUR CODE HERE\n",
    "    return (error_before_split - error_after_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing binary decision tree helper functions from past assignment\n",
    "Recall from the previous assignment that we wrote a function `intermediate_node_num_mistakes` that calculates the number of **misclassified examples** when predicting the **majority class**. This is used to help determine which feature is best to split on at a given node of the tree.\n",
    "\n",
    "**Please copy and paste your code for `intermediate_node_num_mistakes` here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intermediate_node_num_mistakes(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    ## YOUR CODE HERE\n",
    "    num_safe = 0\n",
    "    for node in labels_in_node:\n",
    "        if node == 1:\n",
    "            num_safe += 1\n",
    "    \n",
    "    # Count the number of -1's (risky loans)\n",
    "    ## YOUR CODE HERE\n",
    "    num_risky = 0\n",
    "    for node in labels_in_node:\n",
    "        if node == -1:\n",
    "            num_risky += 1\n",
    "                \n",
    "    # Return the number of mistakes that the majority classifier makes.\n",
    "    ## YOUR CODE HERE\n",
    "    if num_safe > num_risky:\n",
    "        return num_risky\n",
    "    else:\n",
    "        return num_safe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then wrote a function `best_splitting_feature` that finds the best feature to split on given the data and a list of features to consider.\n",
    "\n",
    "**Please copy and paste your `best_splitting_feature` code here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(data))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature] == 0]\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        ## YOUR CODE HERE\n",
    "        right_split = data[data[feature] == 1] \n",
    "            \n",
    "        # Calculate the number of misclassified examples in the left split.\n",
    "        # Remember that we implemented a function for this! (It was called intermediate_node_num_mistakes)\n",
    "        # YOUR CODE HERE\n",
    "        left_mistakes = intermediate_node_num_mistakes(left_split[target])             \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        ## YOUR CODE HERE\n",
    "        right_mistakes = intermediate_node_num_mistakes(right_split[target])\n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of mistakes (left) + # of mistakes (right)) / (# of data points)\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_mistakes + right_mistakes) / num_data_points\n",
    "\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        ## YOUR CODE HERE\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "        \n",
    "    \n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, recall the function `create_leaf` from the previous assignment, which creates a leaf node given a set of target values.  \n",
    "\n",
    "**Please copy and paste your `create_leaf` code here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None,\n",
    "            'is_leaf': True,\n",
    "            'prediction': None}   ## YOUR CODE HERE\n",
    "    \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])\n",
    "    \n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] = 1         ## YOUR CODE HERE\n",
    "    else:\n",
    "        leaf['prediction'] = -1         ## YOUR CODE HERE\n",
    "        \n",
    "    # Return the leaf node        \n",
    "    return leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating new early stopping conditions in binary decision tree implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will implement a function that builds a decision tree handling the three early stopping conditions described in this assignment.  In particular, you will write code to detect early stopping conditions 2 and 3.  You implemented above the functions needed to detect these conditions.  The 1st early stopping condition, **max_depth**, was implemented in the previous assigment and you will not need to reimplement this.  In addition to these early stopping conditions, the typical stopping conditions of having no mistakes or no more features to split on (which we denote by \"stopping conditions\" 1 and 2) are also included as in the previous assignment.\n",
    "\n",
    "**Implementing early stopping condition 2: minimum node size:**\n",
    "\n",
    "* **Step 1:** Use the function **reached_minimum_node_size** that you implemented earlier to write an if condition to detect whether we have hit the base case, i.e., the node does not have enough data points and should be turned into a leaf. Don't forget to use the `min_node_size` argument.\n",
    "* **Step 2:** Return a leaf. This line of code should be the same as the other (pre-implemented) stopping conditions.\n",
    "\n",
    "\n",
    "**Implementing early stopping condition 3: minimum error reduction:**\n",
    "\n",
    "**Note:** This has to come after finding the best splitting feature so we can calculate the error after splitting in order to calculate the error reduction.\n",
    "\n",
    "* **Step 1:** Calculate the **classification error before splitting**.  Recall that classification error is defined as:\n",
    "\n",
    "$$\n",
    "\\text{classification error} = \\frac{\\text{# mistakes}}{\\text{# total examples}}\n",
    "$$\n",
    "* **Step 2:** Calculate the **classification error after splitting**. This requires calculating the number of mistakes in the left and right splits, and then dividing by the total number of examples.\n",
    "* **Step 3:** Use the function **error_reduction** to that you implemented earlier to write an if condition to detect whether  the reduction in error is less than the constant provided (`min_error_reduction`). Don't forget to use that argument.\n",
    "* **Step 4:** Return a leaf. This line of code should be the same as the other (pre-implemented) stopping conditions.\n",
    "\n",
    "Fill in the places where you find `## YOUR CODE HERE`. There are **seven** places in this function for you to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, \n",
    "                         max_depth = 10, min_node_size=1, \n",
    "                         min_error_reduction=0.0):\n",
    "    \n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(\"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values)))\n",
    "\n",
    "    \n",
    "    # Stopping condition 1: All nodes are of the same type.\n",
    "    if intermediate_node_num_mistakes(target_values) == 0:\n",
    "        print(\"Stopping condition 1 reached. All data points have the same target value.\")\n",
    "        return create_leaf(target_values)\n",
    "\n",
    "    # Stopping condition 2: No more features to split on.\n",
    "    if remaining_features == []:\n",
    "        print(\"Stopping condition 2 reached. No remaining features.\")\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Early stopping condition 1: Reached max depth limit.\n",
    "    if current_depth >= max_depth:\n",
    "        print(\"Early stopping condition 1 reached. Reached maximum depth.\")\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Early stopping condition 2: Reached the minimum node size.\n",
    "    # If the number of data points is less than or equal to the minimum size, return a leaf.\n",
    "    if reached_minimum_node_size(data, min_node_size) == True:          ## YOUR CODE HERE \n",
    "        print(\"Early stopping condition 2 reached. Reached minimum node size.\")\n",
    "        return create_leaf(target_values)   ## YOUR CODE HERE\n",
    "    \n",
    "    # Find the best splitting feature\n",
    "    splitting_feature = best_splitting_feature(data, features, target)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    # Early stopping condition 3: Minimum error reduction\n",
    "    # Calculate the error before splitting (number of misclassified examples \n",
    "    # divided by the total number of examples)\n",
    "    error_before_split = intermediate_node_num_mistakes(target_values) / float(len(data))\n",
    "    \n",
    "    # Calculate the error after splitting (number of misclassified examples \n",
    "    # in both groups divided by the total number of examples)\n",
    "    left_mistakes = intermediate_node_num_mistakes(left_split[target]) / float(len(data))   ## YOUR CODE HERE\n",
    "    right_mistakes = intermediate_node_num_mistakes(right_split[target]) / float(len(data))  ## YOUR CODE HERE\n",
    "    error_after_split = (left_mistakes + right_mistakes) / float(len(data))\n",
    "    \n",
    "    # If the error reduction is LESS THAN OR EQUAL TO min_error_reduction, return a leaf.\n",
    "    if  error_reduction(error_before_split, error_after_split) <= min_error_reduction:         ## YOUR CODE HERE\n",
    "        print(\"Early stopping condition 3 reached. Minimum error reduction.\")\n",
    "        return create_leaf(target_values)  ## YOUR CODE HERE \n",
    "    \n",
    "    \n",
    "    remaining_features.remove(splitting_feature)\n",
    "    print(\"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split)))\n",
    "\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, \n",
    "                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)        \n",
    "    \n",
    "    ## YOUR CODE HERE\n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, \n",
    "                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)\n",
    "    \n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade_A',\n",
       " 'grade_B',\n",
       " 'grade_C',\n",
       " 'grade_D',\n",
       " 'grade_E',\n",
       " 'grade_F',\n",
       " 'grade_G',\n",
       " 'term_ 36 months',\n",
       " 'term_ 60 months',\n",
       " 'home_ownership_MORTGAGE',\n",
       " 'home_ownership_OTHER',\n",
       " 'home_ownership_OWN',\n",
       " 'home_ownership_RENT',\n",
       " 'emp_length_1 year',\n",
       " 'emp_length_10+ years',\n",
       " 'emp_length_2 years',\n",
       " 'emp_length_3 years',\n",
       " 'emp_length_4 years',\n",
       " 'emp_length_5 years',\n",
       " 'emp_length_6 years',\n",
       " 'emp_length_7 years',\n",
       " 'emp_length_8 years',\n",
       " 'emp_length_9 years',\n",
       " 'emp_length_< 1 year']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(loans_data.columns)\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37040 data points).\n",
      "Split on feature term_ 36 months. (9419, 27621)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9419 data points).\n",
      "Split on feature grade_A. (9293, 126)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9293 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (126 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (27621 data points).\n",
      "Split on feature grade_D. (22974, 4647)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22974 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4647 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "small_decision_tree = decision_tree_create(train_data, features, 'safe_loans', max_depth = 2, \n",
    "                                        min_node_size = 10, min_error_reduction=0.0)\n",
    "if count_nodes(small_decision_tree) == 7:\n",
    "    print('Test passed!')\n",
    "else:\n",
    "    print('Test failed... try again!')\n",
    "    print('Number of nodes found                :', count_nodes(small_decision_tree))\n",
    "    print('Number of nodes that should be there : 7' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a tree!\n",
    "\n",
    "Now that your code is working, we will train a tree model on the **train_data** with\n",
    "* `max_depth = 6`\n",
    "* `min_node_size = 100`, \n",
    "* `min_error_reduction = 0.0`\n",
    "\n",
    "**Warning**: This code block may take a minute to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37040 data points).\n",
      "Split on feature term_ 36 months. (9419, 27621)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9419 data points).\n",
      "Split on feature grade_A. (9293, 126)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9293 data points).\n",
      "Split on feature grade_B. (8227, 1066)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8227 data points).\n",
      "Split on feature grade_C. (5974, 2253)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5974 data points).\n",
      "Split on feature grade_D. (3891, 2083)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3891 data points).\n",
      "Split on feature grade_E. (1721, 2170)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1721 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2170 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2083 data points).\n",
      "Split on feature grade_E. (2083, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2083 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2253 data points).\n",
      "Split on feature grade_D. (2253, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2253 data points).\n",
      "Split on feature grade_E. (2253, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2253 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1066 data points).\n",
      "Split on feature emp_length_5 years. (986, 80)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (986 data points).\n",
      "Split on feature emp_length_3 years. (900, 86)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (900 data points).\n",
      "Split on feature grade_C. (900, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (900 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (86 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (80 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (126 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (36, 90)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (36 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (90 data points).\n",
      "Early stopping condition 2 reached. Reached minimum node size.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (27621 data points).\n",
      "Split on feature grade_D. (22974, 4647)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22974 data points).\n",
      "Split on feature grade_E. (21728, 1246)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (21728 data points).\n",
      "Split on feature grade_C. (14570, 7158)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (14570 data points).\n",
      "Split on feature grade_F. (14207, 363)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (14207 data points).\n",
      "Split on feature grade_G. (14097, 110)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (14097 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (110 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (363 data points).\n",
      "Split on feature grade_A. (363, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (363 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (7158 data points).\n",
      "Split on feature home_ownership_RENT. (3458, 3700)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3458 data points).\n",
      "Split on feature emp_length_< 1 year. (3257, 201)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3257 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (201 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3700 data points).\n",
      "Split on feature emp_length_2 years. (3279, 421)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3279 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (421 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1246 data points).\n",
      "Split on feature grade_A. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1246 data points).\n",
      "Split on feature grade_B. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1246 data points).\n",
      "Split on feature grade_C. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1246 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4647 data points).\n",
      "Split on feature grade_A. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4647 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_B. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (4647 data points).\n",
      "Split on feature grade_C. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4647 data points).\n",
      "Split on feature grade_E. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4647 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree_new = decision_tree_create(train_data, features, 'safe_loans', max_depth = 6, \n",
    "                                min_node_size = 100, min_error_reduction=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train a tree model **ignoring early stopping conditions 2 and 3** so that we get the same tree as in the previous assignment.  To ignore these conditions, we set `min_node_size=0` and `min_error_reduction=-1` (a negative value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37040 data points).\n",
      "Split on feature term_ 36 months. (9419, 27621)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9419 data points).\n",
      "Split on feature grade_A. (9293, 126)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9293 data points).\n",
      "Split on feature grade_B. (8227, 1066)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (8227 data points).\n",
      "Split on feature grade_C. (5974, 2253)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (5974 data points).\n",
      "Split on feature grade_D. (3891, 2083)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3891 data points).\n",
      "Split on feature grade_E. (1721, 2170)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1721 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2170 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2083 data points).\n",
      "Split on feature grade_E. (2083, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2083 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (2253 data points).\n",
      "Split on feature grade_D. (2253, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (2253 data points).\n",
      "Split on feature grade_E. (2253, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (2253 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1066 data points).\n",
      "Split on feature emp_length_5 years. (986, 80)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (986 data points).\n",
      "Split on feature emp_length_3 years. (900, 86)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (900 data points).\n",
      "Split on feature grade_C. (900, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (900 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (86 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (39, 47)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (39 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (47 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (80 data points).\n",
      "Split on feature home_ownership_RENT. (58, 22)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (58 data points).\n",
      "Split on feature grade_C. (58, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (58 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (22 data points).\n",
      "Split on feature grade_C. (22, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (22 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (126 data points).\n",
      "Split on feature home_ownership_MORTGAGE. (36, 90)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (36 data points).\n",
      "Split on feature emp_length_< 1 year. (26, 10)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (26 data points).\n",
      "Split on feature emp_length_2 years. (23, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (23 data points).\n",
      "Split on feature emp_length_5 years. (20, 3)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (20 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3 data points).\n",
      "Split on feature grade_B. (3, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (10 data points).\n",
      "Split on feature grade_B. (10, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (10 data points).\n",
      "Split on feature grade_C. (10, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (10 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (90 data points).\n",
      "Split on feature grade_B. (90, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (90 data points).\n",
      "Split on feature grade_C. (90, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (90 data points).\n",
      "Split on feature grade_D. (90, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (90 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (27621 data points).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on feature grade_D. (22974, 4647)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (22974 data points).\n",
      "Split on feature grade_E. (21728, 1246)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (21728 data points).\n",
      "Split on feature grade_C. (14570, 7158)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (14570 data points).\n",
      "Split on feature grade_F. (14207, 363)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (14207 data points).\n",
      "Split on feature grade_G. (14097, 110)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (14097 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (110 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (363 data points).\n",
      "Split on feature grade_A. (363, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (363 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (7158 data points).\n",
      "Split on feature home_ownership_RENT. (3458, 3700)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3458 data points).\n",
      "Split on feature emp_length_< 1 year. (3257, 201)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3257 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (201 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (3700 data points).\n",
      "Split on feature emp_length_2 years. (3279, 421)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (3279 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (421 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (1246 data points).\n",
      "Split on feature grade_A. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (1246 data points).\n",
      "Split on feature grade_B. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (1246 data points).\n",
      "Split on feature grade_C. (1246, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (1246 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4647 data points).\n",
      "Split on feature grade_A. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4647 data points).\n",
      "Split on feature grade_B. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (4647 data points).\n",
      "Split on feature grade_C. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (4647 data points).\n",
      "Split on feature grade_E. (4647, 0)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (4647 data points).\n",
      "Early stopping condition 1 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 6 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 5 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 4 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (0 data points).\n",
      "Stopping condition 1 reached. All data points have the same target value.\n"
     ]
    }
   ],
   "source": [
    "my_decision_tree_old = decision_tree_create(train_data, features, 'safe_loans', max_depth = 6, \n",
    "                                min_node_size = 0, min_error_reduction=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print(\"At leaf, predicting %s\" % tree['prediction'])\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print(\"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value))\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "               ### YOUR CODE HERE\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data, target):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    ## YOUR CODE HERE\n",
    "    num_errors = 0\n",
    "    for item in range(len(data)):\n",
    "        if data['safe_loans'][item] != prediction[item]:\n",
    "            num_errors += 1\n",
    "    \n",
    "    return num_errors / float(len(data))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
