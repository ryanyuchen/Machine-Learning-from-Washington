{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 4: Ridge Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement ridge regression via gradient descent. You will:\n",
    "* Convert an SFrame into a Numpy array\n",
    "* Write a Numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "* Write gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19452</th>\n",
       "      <td>3980300371</td>\n",
       "      <td>20140926T000000</td>\n",
       "      <td>142000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>290.0</td>\n",
       "      <td>20875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98024</td>\n",
       "      <td>47.5308</td>\n",
       "      <td>-121.888</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>22850.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15381</th>\n",
       "      <td>2856101479</td>\n",
       "      <td>20140701T000000</td>\n",
       "      <td>276000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>370.0</td>\n",
       "      <td>1801</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>370</td>\n",
       "      <td>0</td>\n",
       "      <td>1923</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6778</td>\n",
       "      <td>-122.389</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>5000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>1723049033</td>\n",
       "      <td>20140620T000000</td>\n",
       "      <td>245000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>380.0</td>\n",
       "      <td>15000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>380</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98168</td>\n",
       "      <td>47.4810</td>\n",
       "      <td>-122.323</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>15000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18379</th>\n",
       "      <td>1222029077</td>\n",
       "      <td>20141029T000000</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>384.0</td>\n",
       "      <td>213444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>384</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98070</td>\n",
       "      <td>47.4177</td>\n",
       "      <td>-122.491</td>\n",
       "      <td>1920.0</td>\n",
       "      <td>224341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4868</th>\n",
       "      <td>6896300380</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>228000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>390.0</td>\n",
       "      <td>5900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>390</td>\n",
       "      <td>0</td>\n",
       "      <td>1953</td>\n",
       "      <td>0</td>\n",
       "      <td>98118</td>\n",
       "      <td>47.5260</td>\n",
       "      <td>-122.261</td>\n",
       "      <td>2170.0</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8092</th>\n",
       "      <td>1924059029</td>\n",
       "      <td>20140617T000000</td>\n",
       "      <td>4668000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>9640.0</td>\n",
       "      <td>13068</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>4820</td>\n",
       "      <td>4820</td>\n",
       "      <td>1983</td>\n",
       "      <td>2009</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5570</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>10454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9254</th>\n",
       "      <td>9208900037</td>\n",
       "      <td>20140919T000000</td>\n",
       "      <td>6885000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>9890.0</td>\n",
       "      <td>31374</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>8860</td>\n",
       "      <td>1030</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98039</td>\n",
       "      <td>47.6305</td>\n",
       "      <td>-122.240</td>\n",
       "      <td>4540.0</td>\n",
       "      <td>42730.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3914</th>\n",
       "      <td>9808700762</td>\n",
       "      <td>20140611T000000</td>\n",
       "      <td>7062500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.50</td>\n",
       "      <td>10040.0</td>\n",
       "      <td>37325</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>7680</td>\n",
       "      <td>2360</td>\n",
       "      <td>1940</td>\n",
       "      <td>2001</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6500</td>\n",
       "      <td>-122.214</td>\n",
       "      <td>3930.0</td>\n",
       "      <td>25449.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7252</th>\n",
       "      <td>6762700020</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>7700000.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>12050.0</td>\n",
       "      <td>27600</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>8570</td>\n",
       "      <td>3480</td>\n",
       "      <td>1910</td>\n",
       "      <td>1987</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6298</td>\n",
       "      <td>-122.323</td>\n",
       "      <td>3940.0</td>\n",
       "      <td>8800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12777</th>\n",
       "      <td>1225069038</td>\n",
       "      <td>20140505T000000</td>\n",
       "      <td>2280000.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>13540.0</td>\n",
       "      <td>307752</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>9410</td>\n",
       "      <td>4130</td>\n",
       "      <td>1999</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6675</td>\n",
       "      <td>-121.986</td>\n",
       "      <td>4850.0</td>\n",
       "      <td>217800.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "19452  3980300371  20140926T000000   142000.0       0.0       0.00   \n",
       "15381  2856101479  20140701T000000   276000.0       1.0       0.75   \n",
       "860    1723049033  20140620T000000   245000.0       1.0       0.75   \n",
       "18379  1222029077  20141029T000000   265000.0       0.0       0.75   \n",
       "4868   6896300380  20141002T000000   228000.0       0.0       1.00   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "8092   1924059029  20140617T000000  4668000.0       5.0       6.75   \n",
       "9254   9208900037  20140919T000000  6885000.0       6.0       7.75   \n",
       "3914   9808700762  20140611T000000  7062500.0       5.0       4.50   \n",
       "7252   6762700020  20141013T000000  7700000.0       6.0       8.00   \n",
       "12777  1225069038  20140505T000000  2280000.0       7.0       8.00   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "19452        290.0     20875     1.0           0     0  ...      1   \n",
       "15381        370.0      1801     1.0           0     0  ...      5   \n",
       "860          380.0     15000     1.0           0     0  ...      5   \n",
       "18379        384.0    213444     1.0           0     0  ...      4   \n",
       "4868         390.0      5900     1.0           0     0  ...      4   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "8092        9640.0     13068     1.0           1     4  ...     12   \n",
       "9254        9890.0     31374     2.0           0     4  ...     13   \n",
       "3914       10040.0     37325     2.0           1     2  ...     11   \n",
       "7252       12050.0     27600     2.5           0     3  ...     13   \n",
       "12777      13540.0    307752     3.0           0     4  ...     12   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "19452         290              0      1963             0    98024  47.5308   \n",
       "15381         370              0      1923             0    98117  47.6778   \n",
       "860           380              0      1963             0    98168  47.4810   \n",
       "18379         384              0      2003             0    98070  47.4177   \n",
       "4868          390              0      1953             0    98118  47.5260   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "8092         4820           4820      1983          2009    98040  47.5570   \n",
       "9254         8860           1030      2001             0    98039  47.6305   \n",
       "3914         7680           2360      1940          2001    98004  47.6500   \n",
       "7252         8570           3480      1910          1987    98102  47.6298   \n",
       "12777        9410           4130      1999             0    98053  47.6675   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "19452 -121.888         1620.0     22850.0  \n",
       "15381 -122.389         1340.0      5000.0  \n",
       "860   -122.323         1170.0     15000.0  \n",
       "18379 -122.491         1920.0    224341.0  \n",
       "4868  -122.261         2170.0      6000.0  \n",
       "...        ...            ...         ...  \n",
       "8092  -122.210         3270.0     10454.0  \n",
       "9254  -122.240         4540.0     42730.0  \n",
       "3914  -122.214         3930.0     25449.0  \n",
       "7252  -122.323         3940.0      8800.0  \n",
       "12777 -121.986         4850.0    217800.0  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_dict = {'bathrooms':float, 'waterfront':int, 'sqft_above':int, 'sqft_living15':float, 'grade':int, 'yr_renovated':int, 'price':float, 'bedrooms':float, 'zipcode':str, 'long':float, 'sqft_lot15':float, 'sqft_living':float, 'floors':float, 'condition':int, 'lat':float, 'date':str, 'sqft_basement':int, 'yr_built':int, 'id':str, 'sqft_lot':int, 'view':int}\n",
    "\n",
    "sales = pd.read_csv('kc_house_data.csv', dtype=dtype_dict)\n",
    "sales = sales.sort_values(['sqft_living','price'])\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful functions from previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_frame, features, output):\n",
    "    data_frame['constant'] = 1 # this is how you add a constant column to data frame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "\n",
    "    # the following line will convert the features and output into a numpy matrix:\n",
    "    features_df = data_frame[features]\n",
    "    feature_matrix = features_df.to_numpy()\n",
    "   \n",
    "    output_df = data_frame[output]\n",
    "    output_array = output_df.to_numpy()\n",
    "\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1. 290.]\n",
      "142000.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') # the [] around 'sqft_living' makes it a list\n",
    "print(example_features[0,:]) # this accesses the first row of the data the ':' indicates 'all columns'\n",
    "print(example_output[0]) # and the corresponding output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
    "    # create the predictions vector by using np.dot()\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "\n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291.0\n",
      "371.0\n"
     ]
    }
   ],
   "source": [
    "my_weights = np.array([1., 1.]) # the example weights\n",
    "test_predictions = predict_output(example_features, my_weights)\n",
    "print(test_predictions[0])\n",
    "print(test_predictions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\n",
    "```\n",
    "Cost(w)\n",
    "= SUM[ (prediction - output)^2 ]\n",
    "+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).\n",
    "```\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives, we can take the derivative of the first part (the RSS) as we did in the notebook for the unregularized case in Week 2 and add the derivative of the regularization part.  As we saw, the derivative of the RSS with respect to `w[i]` can be written as: \n",
    "```\n",
    "2*SUM[ error*[feature_i] ].\n",
    "```\n",
    "The derivative of the regularization term with respect to `w[i]` is:\n",
    "```\n",
    "2*l2_penalty*w[i].\n",
    "```\n",
    "Summing both, we get\n",
    "```\n",
    "2*SUM[ error*[feature_i] ] + 2*l2_penalty*w[i].\n",
    "```\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself, plus `2*l2_penalty*w[i]`. \n",
    "\n",
    "**We will not regularize the constant.**  Thus, in the case of the constant, the derivative is just twice the sum of the errors (without the `2*l2_penalty*w[0]` term).\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors, plus `2*l2_penalty*w[i]`.\n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).  To decide when to we are dealing with the constant (so we don't regularize it) we added the extra parameter to the call `feature_is_constant` which you should set to `True` when computing the derivative of the constant and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    # If feature_is_constant is True, derivative is twice the dot product of errors and feature\n",
    "    \n",
    "    # Otherwise, derivative is twice the dot product plus 2*l2_penalty*weight\n",
    "    \n",
    "    if (feature_is_constant):\n",
    "        derivative = np.dot(2,np.dot(errors, feature))\n",
    "    else:\n",
    "        derivative = np.dot(2,np.dot(errors, feature)) + np.dot(2,np.dot(l2_penalty, weight))\n",
    "    \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-56554166815950.0\n",
      "-56554166815950.0\n",
      "-22446749330.0\n",
      "-22446749330.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([1., 10.])\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "errors = test_predictions - example_output # prediction errors\n",
    "\n",
    "# next two lines should print the same values\n",
    "print(feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False))\n",
    "print(np.sum(errors*example_features[:,1])*2+20.)\n",
    "\n",
    "# next two lines should print the same values\n",
    "print(feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True))\n",
    "print(np.sum(errors)*2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. Unlike in Week 2, this time we will set a **maximum number of iterations** and take gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100 by default. (Use default parameter values in Python.)\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n",
    "    print('Starting gradient descent with l2_penalty = ' + str(l2_penalty))\n",
    "    \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    iteration = 0 # iteration counter\n",
    "    print_frequency = 1  # for adjusting frequency of debugging output\n",
    "    \n",
    "    #while not reached maximum number of iterations:\n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1  # increment iteration counter\n",
    "        ### === code section for adjusting frequency of debugging output. ===\n",
    "        if iteration == 10:\n",
    "            print_frequency = 10\n",
    "        if iteration == 100:\n",
    "            print_frequency = 100\n",
    "        if iteration%print_frequency==0:\n",
    "            print('Iteration = ' + str(iteration))\n",
    "        ### === end code section ===\n",
    "        \n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "\n",
    "        # compute the errors as predictions - output\n",
    "        errors = np.subtract(predictions, output)\n",
    "\n",
    "        # from time to time, print the value of the cost function\n",
    "        if iteration%print_frequency==0:\n",
    "            print('Cost function = ' + str(np.dot(errors,errors) + l2_penalty*(np.dot(weights,weights) - weights[0]**2)))\n",
    "        \n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:,i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i].\n",
    "            #(Remember: when i=0, you are computing the derivative of the constant!)                \n",
    "\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            if (i == 0):\n",
    "                feature_is_constant = True\n",
    "                derivative = feature_derivative_ridge(errors, feature_matrix[:, i], weights[i], l2_penalty, feature_is_constant)\n",
    "                weights[i] = weights[i] - (step_size*derivative)\n",
    "            else:\n",
    "                feature_is_constant = False\n",
    "                derivative = feature_derivative_ridge(errors, feature_matrix[:, i], weights[i], l2_penalty, feature_is_constant)\n",
    "                weights[i] = weights[i] - (step_size*derivative)\n",
    "            \n",
    "    print('Done with gradient descent at iteration: ' + str(iteration))\n",
    "    print('Learned weights = ' + str(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing effect of L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let's see how large weights get penalized. Let us consider a simple model with 1 feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(sales, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0., 0.])\n",
    "step_size = 1e-12\n",
    "max_iterations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider no regularization.  Set the `l2_penalty` to `0.1` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`simple_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_penalty = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0.1\n",
      "Iteration = 1\n",
      "Cost function = 9217325138472070.0\n",
      "Iteration = 2\n",
      "Cost function = 6147345564844980.0\n",
      "Iteration = 3\n",
      "Cost function = 4296084086077826.5\n",
      "Iteration = 4\n",
      "Cost function = 3179734944131010.0\n",
      "Iteration = 5\n",
      "Cost function = 2506553213161872.0\n",
      "Iteration = 6\n",
      "Cost function = 2100610640270264.8\n",
      "Iteration = 7\n",
      "Cost function = 1855818833073353.2\n",
      "Iteration = 8\n",
      "Cost function = 1708204286390232.0\n",
      "Iteration = 9\n",
      "Cost function = 1619189647192850.0\n",
      "Iteration = 10\n",
      "Cost function = 1565511969947073.8\n",
      "Iteration = 20\n",
      "Cost function = 1484492732774825.5\n",
      "Iteration = 30\n",
      "Cost function = 1483977608843702.5\n",
      "Iteration = 40\n",
      "Cost function = 1483974332729873.2\n",
      "Iteration = 50\n",
      "Cost function = 1483974310961413.0\n",
      "Iteration = 60\n",
      "Cost function = 1483974309884188.5\n",
      "Iteration = 70\n",
      "Cost function = 1483974308938519.5\n",
      "Iteration = 80\n",
      "Cost function = 1483974307993687.0\n",
      "Iteration = 90\n",
      "Cost function = 1483974307048860.0\n",
      "Iteration = 100\n",
      "Cost function = 1483974306104033.5\n",
      "Iteration = 200\n",
      "Cost function = 1483974296655773.2\n",
      "Iteration = 300\n",
      "Cost function = 1483974287207526.5\n",
      "Iteration = 400\n",
      "Cost function = 1483974277759292.8\n",
      "Iteration = 500\n",
      "Cost function = 1483974268311072.5\n",
      "Iteration = 600\n",
      "Cost function = 1483974258862865.8\n",
      "Iteration = 700\n",
      "Cost function = 1483974249414672.0\n",
      "Iteration = 800\n",
      "Cost function = 1483974239966491.5\n",
      "Iteration = 900\n",
      "Cost function = 1483974230518324.8\n",
      "Iteration = 1000\n",
      "Cost function = 1483974221070171.2\n",
      "Done with gradient descent at iteration: 1000\n",
      "Learned weights = [-2.01527531e-01  2.63089270e+02]\n"
     ]
    }
   ],
   "source": [
    "simple_weights = ridge_regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=max_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1caeb05ac08>,\n",
       " <matplotlib.lines.Line2D at 0x1caeb078c88>,\n",
       " <matplotlib.lines.Line2D at 0x1caeb08fbc8>,\n",
       " <matplotlib.lines.Line2D at 0x1caeb08fdc8>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(simple_feature_matrix,output,'k.',\n",
    "         simple_feature_matrix,predict_output(simple_feature_matrix, simple_weights),'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
